{
 "activation_dropout": 0.0,
 "activation_function": "relu",
 "attention_dropout": 0.0,
 "bos_token_id": 2,
 "dropout": 0.1,
 "eos_token_id": 2,
 "init_std": 0.02,
 "layerdrop": 0.0,
 "max_position_embeddings": 2048,
 "model_type": "opt",
 "pad_token_id": 1,
 "use_cache": true,
 "weight_sharing": false,
 "vocab_size": 50257,
 "word_embed_proj_dim": 768,
 "head_dim": 64,
 "ffn_dim": [
  640,
  1152,
  896,
  1024
 ],
 "hidden_size": [
  640,
  768,
  896,
  1024
 ],
 "layers_per_block": [
  2,
  2,
  2,
  2
 ],
 "num_blocks": 4
}